{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DiffRoute Overview","text":"<p><code>diffroute</code> provides a differentiable and GPU-accelerated formulation of classical Linear Time Invariant (LTI) river routing models.  Every component is written in PyTorch, which keeps routing differentiable and easy to integrate in deep learning workflows.</p> <p>For advanced use-cases, we recommend using diffroute through the companion module <code>diffhydro</code>,  which layers additional data structures and utilities to integrate DiffRoute into more complex differentiable hydrological pipelines, with minimal dependency. However, DiffRoute can also be used as a standalone component as showcased in this documentation.</p> <p>Please note that <code>diffroute</code> is in an early development stage and thus subject to changes. Nevertheless, the explanations included in this documentation are limited to API and structural components that are unlikely to change in the short to middle term.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>GPU Acceleration: Formulating LTI River Routing operations as 1D Convolution layers allow for efficient GPU execution.</li> <li>Differentiable: DiffRoute is integrated to Pytorch, which allows for efficient gradient computations through Automatic Differentiation.</li> <li>Scalable: Computations scale to very large river graphs (up to millions of river channels) using staged computations.</li> <li>Generality: DiffRoute allows to formulate any LTI River Routing scheme. Currenlty implemented routing schemes include the Muskingum, Linear Diffusive Wave, Linear Storage, Nash Cascade and Pure Lag schemes. Custom schemes can easily be added as shown in this example. </li> <li>Batching: Accepts batched runoff tensors with shape <code>[batch, catchments, time]</code> and routes them efficiently. This simplifies batched training of parameters and inference of ensemble predictions.</li> <li>Composability: Integration to Pytorch Automatic Differentiation framework also aims to combine <code>diffroute</code> with other learnable components. The companion module <code>diffhydro</code> aims to simplify the assembly of more complex hydrological pipelines.</li> <li>Pure Python: DiffRoute is written in pure python. As such it is easily hackable for users interested in experimenting with variations.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can install <code>diffroute</code> from pip repository:</p> <pre><code>pip install diffroute\n</code></pre> <p>Or install the latest version from GitHub: <pre><code>pip install git+https://github.com/TristHas/DiffRoute.git\n</code></pre></p> <p>Or clone and install locally:</p> <pre><code>git clone https://github.com/TristHas/DiffRoute.git\ncd DiffRoute; pip install .\n</code></pre>"},{"location":"#example-usage","title":"Example Usage","text":"<p>Below is a toy example for routing 2 realisations of 100 time steps random input runoff time series through a synthetic river graph of 20 channels.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport torch\n\nfrom diffroute import RivTree, LTIRouter\n\nb = 2   # Batch (or ensemble size)\nn = 20  # Number of channels\nT = 100 # Number of time steps\ndevice = \"cuda:0\" # GPU device to use\n\nG = nx.gn_graph(n) # Toy example tree with a unique outlet.\n# Define per-node routing parameters. For a Linear Storage scheme, only one parameter \"tau\"\nparams = pd.DataFrame({\"tau\": np.random.rand(n)}, index=G.nodes) \n# River Tree data structure\nriv_tree = RivTree(G, params=params, irf_fn=\"linear_storage\").to(device) \n# Generate random input runoff\nrunoff = torch.rand(b, n, T, device=device)\n# Instantiate the routing model with desired parameters\nrouter = LTIRouter(max_delay=48, dt=1)\n# Compute output discharges from input graph and runoffs\ndischarge = router(runoff, riv_tree)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>We recommend first having a look at the Quickstart page for additional explanations on the above code snippet.</p> <p>The Concepts section introduces the main data structures and workflows provided by DiffRoute.</p> <p>Details on repository structure and API are laid out in Code Structure and the API Reference.</p> <p>The Examples section provides two tutorial notebooks for (i) accelerating existing RAPID simulations on GPU and (ii) integrating custom LTI schemes.</p> <p>For advanced usage examples and integration of DiffRoute into larger differentiable hydrological pipelines, please refer to the DiffHydro documentation. </p>"},{"location":"#contribution","title":"Contribution","text":"<p>This project is under early development efforts. Please be aware that some changes may be brought to the API in future versions. We appreciate all contributions, from reporting bugs to implementing new features.  Please use the GitHub Issues for any contribution, comment or request.</p>"},{"location":"#license-and-citation","title":"License and citation","text":"<p>This project is licensed under the terms of the MIT license.  </p> <p>The motivation for, mathematical derivations behind, and illustrative use-cases of DiffRoute are presented in the paper \"Differentiable river routing for end-to-end learning of hydrological processes\".</p> <p>\"Differentiable river routing for end-to-end learning of hydrological processes at diverse scales\" ESS Open Archive . May 27, 2025. DOI: 10.22541/essoar.174835108.87664030/v1</p> <p>If you use <code>diffroute</code> in your academic work, please cite the above reference.</p>"},{"location":"#next","title":"Next","text":"<p>Continue with the Quickstart to see a minimal runnable example and recommended configuration patterns.</p>"},{"location":"api/","title":"API Reference","text":"<p>The sections below are generated from the package docstrings using <code>mkdocstrings</code>. Every time the site is built, the reference stays up to date with the latest type hints and Google-style docstrings.</p>"},{"location":"api/#ltirouter","title":"LTIRouter","text":"<p>               Bases: <code>Module</code></p> <p>Linear time-invariant river routing module.</p> <p>Combines an impulse response function aggregator with a block-sparse convolution to transform runoff into downstream discharge.</p>"},{"location":"api/#diffroute.router.LTIRouter.__init__","title":"__init__","text":"<pre><code>__init__(max_delay=100, dt=1, sampling_mode='avg', block_size=16, block_f=128, cascade=1, **kwargs)\n</code></pre> <p>Initialize the router with aggregation and convolution settings.</p> <p>Parameters:</p> Name Type Description Default <code>max_delay</code> <code>int</code> <p>Maximum impulse response length in time-steps.</p> <code>100</code> <code>dt</code> <code>float</code> <p>Temporal resolution of the runoff inputs.</p> <code>1</code> <code>sampling_mode</code> <code>str</code> <p>Strategy for sampling cascade parameters.</p> <code>'avg'</code> <code>block_size</code> <code>int</code> <p>Spatial block size used for block-sparse kernels.</p> <code>16</code> <code>block_f</code> <code>int</code> <p>Hidden dimensionality for kernel factorization.</p> <code>128</code> <code>cascade</code> <code>int</code> <p>Number of cascaded IRFs combined by the aggregator.</p> <code>1</code> <code>**kwargs</code> <p>Unused keyword arguments kept for legacy compatibility.</p> <code>{}</code>"},{"location":"api/#diffroute.router.LTIRouter.forward","title":"forward","text":"<pre><code>forward(runoff, g, params=None)\n</code></pre> <p>Compute routed discharge for a set of runoff inputs.</p> <p>Parameters:</p> Name Type Description Default <code>runoff</code> <code>Tensor</code> <p>Tensor shaped <code>[B, C, T]</code> with batch, channel (node), and time dimensions.</p> required <code>g</code> <code>RivTree</code> <p>River network containing kernel parameters.</p> required <code>params</code> <code>Tensor | None</code> <p>Optional per-cluster parameters; defaults to attributes stored on <code>g</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Routed discharge with shape <code>[B, C, T]</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>runoff</code> does not have three dimensions.</p>"},{"location":"api/#ltistagedrouting","title":"LTIStagedRouting","text":"<p>               Bases: <code>Module</code></p> <p>Staged router that orchestrates block-sparse routing over clusters.</p> <p>Wraps <code>LTIRouter</code> while managing inter-cluster transfers entirely in PyTorch tensors for batched execution.</p> <p>               Bases: <code>Module</code></p> <p>Collection of river subgraphs with optional inter-cluster transfers.</p>"},{"location":"api/#diffroute.staged_router.LTIStagedRouter.__init__","title":"__init__","text":"<pre><code>__init__(max_delay=32, dt=1.0, sampling_mode='avg', block_size=16, block_f=128, cascade=1)\n</code></pre> <p>Configure the staged router and construct its base LTI model.</p> <p>Parameters:</p> Name Type Description Default <code>max_delay</code> <code>int</code> <p>Maximum impulse response length in time-steps.</p> <code>32</code> <code>dt</code> <code>float</code> <p>Temporal resolution of the runoff inputs.</p> <code>1.0</code> <code>sampling_mode</code> <code>str</code> <p>Strategy for sampling cascade parameters.</p> <code>'avg'</code> <code>block_size</code> <code>int | None</code> <p>Optional override for kernel block size.</p> <code>16</code> <code>block_f</code> <code>int</code> <p>Hidden dimensionality for kernel factorization.</p> <code>128</code> <code>cascade</code> <code>int</code> <p>Number of cascaded IRFs combined by the aggregator.</p> <code>1</code>"},{"location":"api/#diffroute.staged_router.LTIStagedRouter.route_one_cluster","title":"route_one_cluster","text":"<pre><code>route_one_cluster(runoff, gs, cluster_idx, params=None, transfer_bucket=None)\n</code></pre> <p>Route a single cluster and update the transfer bucket.</p> <p>Parameters:</p> Name Type Description Default <code>runoff</code> <code>Tensor</code> <p>Cluster runoff shaped <code>[B, n_c, T]</code>.</p> required <code>gs</code> <p>Clustered river structure providing routing metadata.</p> required <code>cluster_idx</code> <code>int</code> <p>Identifier of the cluster to route.</p> required <code>params</code> <code>Any | None</code> <p>Optional parameter bundle for the cluster.</p> <code>None</code> <code>transfer_bucket</code> <code>Tensor | None</code> <p>Global transfer storage.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple[torch.Tensor, torch.Tensor | None]: Routed discharge for</p> <code>Tensor | None</code> <p>the cluster and the updated transfer bucket.</p>"},{"location":"api/#diffroute.staged_router.LTIStagedRouter.route_all_clusters","title":"route_all_clusters","text":"<pre><code>route_all_clusters(x, gs, params=None, display_progress=False)\n</code></pre> <p>Route all clusters sequentially and assemble the full discharge.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Full runoff tensor shaped <code>[B, N_nodes, T]</code>.</p> required <code>gs</code> <p>Clustered river structure with index metadata.</p> required <code>params</code> <code>List[Any] | None</code> <p>Optional per-cluster parameter list.</p> <code>None</code> <code>display_progress</code> <code>bool</code> <p>Whether to wrap the loop with a tqdm bar.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Routed discharge tensor shaped <code>[B, N_nodes, T]</code>.</p>"},{"location":"api/#diffroute.staged_router.LTIStagedRouter.route_all_clusters_yield","title":"route_all_clusters_yield","text":"<pre><code>route_all_clusters_yield(xs, gs, params=None, display_progress=False)\n</code></pre> <p>Yield per-cluster discharges lazily for streamed routing.</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>List[Tensor]</code> <p>Sequence of cluster runoff tensors.</p> required <code>gs</code> <p>Clustered river structure with transfer metadata.</p> required <code>params</code> <code>List[Any] | None</code> <p>Optional per-cluster parameter list.</p> <code>None</code> <code>display_progress</code> <code>bool</code> <p>Whether to wrap iteration in tqdm.</p> <code>False</code> <p>Yields:</p> Type Description <p>torch.Tensor: Discharge tensor for each cluster in order.</p>"},{"location":"api/#diffroute.staged_router.LTIStagedRouter.init_upstream_discharges","title":"init_upstream_discharges","text":"<pre><code>init_upstream_discharges(xs, gs, cluster_idx, params=None, display_progress=False)\n</code></pre> <p>Warm up the staged router until the target cluster.</p> <p>Parameters:</p> Name Type Description Default <code>xs</code> <code>List[Tensor]</code> <p>Sequence of cluster runoff tensors.</p> required <code>gs</code> <p>Clustered river structure with transfer metadata.</p> required <code>cluster_idx</code> <code>int</code> <p>Cluster index to stop before routing.</p> required <code>params</code> <code>List[Any] | None</code> <p>Optional per-cluster parameter list.</p> <code>None</code> <code>display_progress</code> <code>bool</code> <p>Whether to wrap iteration in tqdm.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Transfer bucket capturing upstream discharges.</p>"},{"location":"api/#diffroute.structs.riv_graphs.RivTreeCluster.__init__","title":"__init__","text":"<pre><code>__init__(clusters_g, node_transfer, irf_fn=None, include_index_diag=True, param_df=None, nodes_idx=None)\n</code></pre> <p>Assemble clustered river networks and transfer bookkeeping.</p> <p>Parameters:</p> Name Type Description Default <code>clusters_g</code> <code>Sequence[DiGraph]</code> <p>Clustered river graphs.</p> required <code>node_transfer</code> <code>Dict[int, List[Tuple[int, int, int]]] | None</code> <p>Mapping describing inter-cluster transfers.</p> required <code>irf_fn</code> <code>str | None</code> <p>Name of the IRF parameterization to use.</p> <code>None</code> <code>include_index_diag</code> <code>bool</code> <p>Whether kernels include self-loops.</p> <code>True</code> <code>param_df</code> <code>DataFrame | None</code> <p>Optional parameter table.</p> <code>None</code> <code>nodes_idx</code> <code>Sequence[Series] | None</code> <p>Custom node orderings.</p> <code>None</code>"},{"location":"api/#graph-utilities","title":"Graph Utilities","text":""},{"location":"code_structure/","title":"Code Structure","text":"<p>DiffRoute is organised by routing stage, with clear separation between high-level orchestration, sparse tensor operators, and supporting utilities. The overview below maps each top-level package or module to its role in the pipeline.</p> <ul> <li> <p><code>diffroute/agg</code>   Implements the first stage of routing: building impulse-response kernels. The main class, <code>IRFAggregator</code>, runs an FFT-based transitive-closure to accumulate upstream IRFs, rescales them with <code>SubResolutionSampler</code>, and outputs a <code>SparseKernel</code>. Its <code>forward</code> method accepts a <code>RivTree</code> (plus optional parameter overrides) and returns the aggregated kernel ready for convolution.</p> </li> <li> <p><code>diffroute/conv</code>   Hosts the second stage: block-sparse convolutions. <code>BlockSparseCausalConv.forward</code> expects the runoff tensor and a <code>BlockSparseKernel</code>, then calls either the Triton implementation (<code>diffroute.ops.block_sparse_conv_1d</code>) or a PyTorch fallback. The layer preserves causality and trims padding so outputs match the input horizon.</p> </li> <li> <p><code>diffroute/ops</code>   Contains low-level kernels and graph algorithms. <code>transitive_closure</code> powers stage one by summing IRFs in the Fourier domain, while convolution routines (Triton and PyTorch variants) drive stage two. Use these if you need custom operator extensions or want to experiment with alternative kernels.</p> </li> <li> <p><code>diffroute/structs</code>   Provides data containers for river graphs (<code>RivTree</code>, <code>RivTreeCluster</code>) and routing kernels (<code>SparseKernel</code>, <code>BlockSparseKernel</code>). Utility helpers cover index initialisation, COO/blocked conversions, and transfer table construction for staged routing.</p> </li> <li> <p><code>diffroute/graph_utils</code>   Offers helpers for graph segmentation, currently centred around <code>define_schedule</code>. The function splits large river networks into stageable clusters and returns both the subgraphs and the inter-cluster transfer map. Expect more clustering strategies here in future releases.</p> </li> <li> <p><code>diffroute/io.py</code>   Handles RAPID interoperability. <code>read_rapid_graph</code> and <code>read_multiple_rapid_graphs</code> parse RAPID CSV exports, attach Muskingum parameters, and return ready-to-route <code>RivTree</code> or <code>RivTreeCluster</code> objects.</p> </li> <li> <p><code>diffroute/irfs.py</code>   Defines the built-in impulse-response functions (Muskingum, Nash cascade, linear storage, Hayami, pure lag) and exposes the <code>register_irf</code> hook for custom kernels.</p> </li> <li> <p><code>diffroute/router.py</code>   Implements <code>LTIRouter</code>, the single-stage operator that combines <code>IRFAggregator</code> and <code>BlockSparseCausalConv</code> inside a PyTorch <code>nn.Module</code>.</p> </li> <li> <p><code>diffroute/staged_router.py</code>   Implements <code>LTIStagedRouter</code>, which orchestrates sequential routing over <code>RivTreeCluster</code> instances, moves discharge between clusters, and reuses the single-stage router internally.</p> </li> </ul>"},{"location":"quickstart/","title":"DiffRoute Quickstart","text":"<p>This walkthrough unpacks the Overview code snippet step by step with deeper explanations.  It shows an example of routing synthetic runoff through a compact river network using <code>diffroute</code>, but the same pattern can applied to real catchments and calibrated parameters.</p>"},{"location":"quickstart/#1-install-and-import","title":"1. Install and import","text":"<pre><code>pip install git+https://github.com/TristHas/DiffRoute.git\n</code></pre> <p>Installs the latest version of <code>diffroute</code> from GitHub; PyTorch, NetworkX, and the remaining dependencies are automatically installed.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport torch\n\nfrom diffroute import RivTree, LTIRouter\n</code></pre>"},{"location":"quickstart/#2-describe-the-river-network","title":"2. Describe the river network","text":"<p>Define the toy routing experiment by choosing batch size, number of reaches, simulation length, and target device. The device parameter is a torch parameter setting on which GPU the computations are carried.</p> <pre><code>b = 2       \nn = 20       \nT = 100      \ndevice = \"cuda:0\"  \n</code></pre> <p>DiffRoute uses NetworkX to describe river graph connectivity. Any connectivity format can be easily read into a Networkx graph. <code>nx.gn_graph</code> quickly supplies a directed tree with a single outlet, which is perfect for demonstration purposes.</p> <pre><code>G = nx.gn_graph(n, seed=0)\n</code></pre> <p>Routing also requires per-channel parameters. The set of parameters required depends on the routing scheme used. Here, we use the <code>linear_storage</code> scheme, which parameterizes channels with a unique parameter <code>tau</code>.  We randomly sample <code>tau</code> values for the sake of illustration.</p> <pre><code>params = pd.DataFrame({\"tau\": np.random.rand(n)}, index=G.nodes)\n</code></pre> <p>The <code>RivTree</code> class is <code>diffroute</code>'s native representations of river graphs. This structure holds the graph connectivity, routing scheme and parameters as torch tensors. As <code>RivTree</code> holds the tensor representation of the graph, it needs to be given a <code>device</code> for storage through the helper <code>.to(device)</code>. It can be instantiated from a NetworkX graph for connectivity and pandas DataFrame for parameters.</p> <pre><code>riv_tree = RivTree(G, params=params, irf_fn=\"linear_storage\").to(device)\n</code></pre>"},{"location":"quickstart/#3-instantiate-the-router","title":"3. Instantiate the router","text":"<p><code>LTIRouter</code> is the primary PyTorch module provided by DiffRoute and can be dropped into any nn.Module graph. You configure it once with routing hyper-parameters: <code>max_delay</code> controls the temporal support of the aggregated kernel,  and <code>dt</code> sets the routing resolution relative to the runoff time step.</p> <pre><code>router = LTIRouter(\n    max_delay=48,  # Time steps to cover all upstream travel times\n    dt=1           # Routing resolution relative to runoff resolution\n)\n</code></pre>"},{"location":"quickstart/#4-route-runoff","title":"4. Route runoff","text":"<p>The router consumes input runoff tensor and <code>RivTree</code> structure to output a discharge tensor. Both inputs must reside on the same device, or an error will be raised. Runoff tensors follow the <code>[batch, catchments, time]</code> layout. The output discharge is a tensor on the same device as the input and with the same shape as the input runoff. The output discharge is fully differentiable with respect to both input runoff and RivTree parameters.</p> <pre><code>runoff = torch.rand(b, n, T, device=device)\ndischarge = router(runoff, riv_tree)\nprint(discharge.shape)\n</code></pre>"},{"location":"quickstart/#next","title":"Next","text":"<ul> <li>Go through the Concepts section for deeper explanations of the core abstractions and configuration details.</li> <li>Browse the Examples section for end-to-end workflows that cover IO, execution at scale, and custom IRF integration.</li> <li>Visit the <code>diffhydro</code> documentation for advanced learning, calibration, and larger pipeline integrations.</li> </ul>"},{"location":"concepts/data_structures/","title":"Data Structures","text":"<p>DiffRoute pairs differentiable operators with light-weight data structures that describe river graphs and sparse routing kernels.  Understanding these containers makes it easier to efficiently use and tweak the library.</p>"},{"location":"concepts/data_structures/#river-graph-representation","title":"River graph representation","text":""},{"location":"concepts/data_structures/#rivtree","title":"<code>RivTree</code>","text":"<ul> <li>Wraps a directed acyclic <code>networkx.DiGraph</code> and precomputes the tensors needed for routing (edge lists, path prefixes, parameter matrices).</li> <li>Defines an ordering of the nodes. This ordering can either be set at initialization (<code>nodes_idx</code>) or defaults to a depth-first traversal if not provided.</li> <li>Stores per-reach IRF parameters in <code>RivTree.params</code>, a floating tensor shaped <code>[n_channels, n_params]</code>.</li> </ul>"},{"location":"concepts/data_structures/#rivtreecluster","title":"<code>RivTreeCluster</code>","text":"<ul> <li>Bundles multiple <code>RivTree</code> instances produced by graph segmentation.</li> <li>Maintains <code>node_ranges</code>, a lookup that maps each cluster to the slice of global reach indices it owns.</li> <li>Tracks inter-cluster transfers through <code>src_transfer</code>, <code>dst_transfer</code>, and <code>tot_transfer</code> buffers so staged routing can move discharge between clusters.</li> <li>Offers <code>__iter__</code> and <code>__getitem__</code> helpers to iterate over constituent <code>RivTree</code>s.</li> </ul>"},{"location":"concepts/data_structures/#routing-kernel-representations","title":"Routing kernel representations","text":""},{"location":"concepts/data_structures/#sparsekernel","title":"<code>SparseKernel</code>","text":"<ul> <li>Stores routing kernels in coordinate (COO) form with <code>coords</code> <code>[nnz, 2]</code>, <code>vals</code> <code>[nnz, window]</code>, and <code>size</code> <code>(rows, cols, window)</code>.</li> <li>Returned by <code>IRFAggregator</code> and easy to convert to dense tensors via <code>to_dense()</code>.</li> <li>Provides <code>.to_block_sparse(block_size)</code> to build a block-structured view.</li> </ul>"},{"location":"concepts/data_structures/#blocksparsekernel","title":"<code>BlockSparseKernel</code>","text":"<ul> <li>Packs kernels into block-sparse tiles that align with GPU-friendly convolution implementations.</li> <li>Keeps <code>block_indices</code> <code>[n_blocks, 2]</code>, <code>block_values</code> <code>[n_blocks, block_size, block_size, window]</code>, and the overall <code>size</code>.</li> <li>Supports conversions: <code>from_sparse_kernel</code>, <code>from_coo</code>, <code>to_dense</code>, and <code>to_coo</code>.</li> <li>The staged convolution layer (<code>BlockSparseCausalConv</code>) consumes this representation directly.</li> </ul> <p>Together, these structures bridge the gap between graph-based hydrological metadata and efficient tensor operations, enabling scalable routing on modern accelerators.</p>"},{"location":"concepts/io/","title":"IO Utilities","text":"<p><code>diffroute</code> can ingest RAPID routing configurations and convert them into ready-to-route <code>RivTree</code> or <code>RivTreeCluster</code> instances.  The helpers live in <code>diffroute.io</code> and handle both single-VPU and multi-VPU projects.</p>"},{"location":"concepts/io/#reading-a-single-rapid-project","title":"Reading a single RAPID project","text":"<pre><code>from pathlib import Path\n\nfrom diffroute.io import read_rapid_graph\n\nvpu_root = Path(\"/path/to/rapid/VPU1234\")\n\n# Without segmentation: returns a single RivTree\ng = read_rapid_graph(vpu_root)\n\n# With segmentation: returns a RivTreeCluster\nclustered = read_rapid_graph(\n    vpu_root,\n    plength_thr=50_000,\n    node_thr=1_000\n)\n\nprint(type(g), len(g))\nprint(type(clustered), len(clustered))\n</code></pre> <ul> <li>RAPID parameters (<code>k</code>, <code>x</code>) are converted to Muskingum IRF coefficients and stored in the resulting data structure.</li> <li>When <code>plength_thr</code> and <code>node_thr</code> are provided, the function runs the graph segmentation pipeline (<code>define_schedule</code>) before building the <code>RivTreeCluster</code>.</li> <li>When <code>plength_thr</code> and <code>node_thr</code> are set as <code>None</code>, staging is omitted and a <code>RivTree</code> instance is returned.</li> </ul>"},{"location":"concepts/io/#reading-multiple-rapid-projects-at-once","title":"Reading multiple RAPID projects at once","text":"<p>Large problems are often split into sub-clusters in RAPID. <code>diffroute</code> provides a helper function to load arbitrarily many such directories and schedule across these clusters as a single river network. </p> <pre><code>from pathlib import Path\n\nfrom diffroute.io import read_multiple_rapid_graphs\n\nvpu_roots = [\n    Path(\"/path/to/rapid/VPU0101\"),\n    Path(\"/path/to/rapid/VPU0102\"),\n]\n\ngs = read_multiple_rapid_graphs(\n    vpu_roots,\n    plength_thr=40_000,\n    node_thr=800\n)\n\nprint(f\"Global reaches: {len(gs.nodes_idx)}\")\n</code></pre> <p><code>read_multiple_rapid_graphs</code> merges the VPU graphs into a single NetworkX DAG, combines their Muskingum parameters, and applies the same optional segmentation logic as the single-project loader.</p>"},{"location":"concepts/multi_stage/","title":"Staged Routing","text":"<p>Large basins quickly push the LTIRouter routing procedure to the limits of GPU memory. This is because the kernel aggregation stage consists in computing the transitive closure of a river graph, which, in dense form, grows as the square of the number of reaches. The block-sparse representation of the kernel helps reduce memory and computational burden of the kernel representation and consequent operations. However, for extremely large river networks, even the sparse representation of the kernel is too heavy for modern GPU memory capacity.</p> <p>To adress this problem, <code>diffroute</code> exposes a staged routing functionality:  large river networks are segmented into clusters. Each cluster is routed sequentially according to their topological order (upstream clusters first). The output of upstream clusters is fed as input to the routing of downstream clusters through transfer buffers.</p> <p>This functionality is exposed through the <code>LTIStagedRouter</code> torch module and <code>RivTreeCluster</code> structure.</p>"},{"location":"concepts/multi_stage/#why-staging-matters","title":"Why staging matters","text":"<p>The worst-case memory complexity of a dense routing kernel is \\(O(N^2 \\times W / dt)\\), where:</p> <ul> <li>\\(N\\) is the number of reaches (graph nodes),</li> <li>\\(W\\) is the impulse response window expressed in the runoff time step unit (e.g. 10 days for daily runoffs),</li> <li>\\(dt\\) is the routing temportal resolution relative to runoff (e.g. 1/24 for hourly routing of daily runoffs).</li> </ul> <p>Doubling the graph size can up-to-quadruple the kernel footprint before considering temporal expansion.  For continental-scale networks at fine spatial resolution, this becomes intractable.  Staging keeps each subgraph small so the per-cluster kernels fit comfortably in GPU memory.</p>"},{"location":"concepts/multi_stage/#segmenting-the-graph","title":"Segmenting the graph","text":"<p><code>diffroute</code> ships a basic segmentation utility, <code>diffroute.graph_utils.define_schedule</code>, that: 1. Scans the directed acyclic river graph and flags breakpoints when upstream routing paths exceed the <code>plength_thr</code> threshold. 2. Cuts breakpoint edges to produce weakly connected components. 3. Groups components into clusters that respect the <code>node_thr</code> size limit. 4. Tracks transfers between clusters so routed discharge can flow downstream.</p> <p>The segmentation produces: - <code>clusters_g</code>: a list of NetworkX subgraphs (one per cluster). - <code>node_transfer</code>: a dictionary describing which nodes exchange discharge across cluster boundaries.</p> <p>A <code>RivTreeCluster</code> object can be instantiated from <code>clusters_g</code> and <code>node_transfer</code>, and directly consumed by <code>LTIStagedRouter</code> that will schedule the routing through the different sub-graphs.</p>"},{"location":"concepts/multi_stage/#working-with-ltistagedrouter","title":"Working with <code>LTIStagedRouter</code>","text":"<p>The staged router wraps a standard <code>LTIRouter</code> and reuses the same IRF catalogue and parameters. A typical workflow is shown below.</p> <pre><code>import networkx as nx\nimport torch\n\nfrom diffroute import LTIStagedRouter, RivTreeCluster\nfrom diffroute.graph_utils import define_schedule\n\ndevice = \"cuda:0\"\n# 1. Build or load the full river network (must be acyclic)\nG = nx.DiGraph()\n# ... populate nodes with IRF parameters and edges with delays ...\n\n# 2. Segment the graph into manageable clusters\nclusters_g, node_transfer = define_schedule(\n    G,\n    plength_thr=25_000,   # breakpoint when cumulative path length exceeds this\n    node_thr=800          # maximum reaches per cluster\n)\n\n# 3. Wrap the segmented network in a RivTreeCluster\ngs = RivTreeCluster(\n    clusters_g,\n    node_transfer=node_transfer,\n    irf_fn=\"linear_storage\",\n    include_index_diag=False\n).to(device)\n\n# 4. Route runoff in stages\nrouter = LTIStagedRouter(\n    max_delay=72,\n    dt=1.0,\n    block_size=16,\n    block_f=128\n)\n\nrunoff = torch.rand(2, len(gs.nodes_idx), 168, device=device)  # [batch, reaches, time]\ndischarge = router(runoff, gs)\nprint(discharge.shape)\n</code></pre>"},{"location":"concepts/multi_stage/#choosing-segmentation-thresholds","title":"Choosing segmentation thresholds","text":"<ul> <li>Start with <code>node_thr</code> in the 500\u20131,000 range for GPUs with 24 GB of memory. Lower values trade more staging passes for smaller kernels.</li> <li>Set <code>plength_thr</code> high enough to let most tributaries stay intact, but low enough to break long serial chains that inflate the kernel width.</li> <li>For complex basins, consider manual seeding (e.g., by pre-clustering with hydrological regions) before running <code>define_schedule</code>.</li> </ul>"},{"location":"concepts/multi_stage/#visualising-the-segmentation","title":"Visualising the segmentation","text":"<p>To inspect the clustering outcome, iterate over <code>clusters_g</code> and plot each subgraph, or summarise them as shown:</p> <pre><code>for cid, cluster in enumerate(clusters_g):\n    n_nodes = cluster.number_of_nodes()\n    n_edges = cluster.number_of_edges()\n    print(f\"Cluster {cid:02d}: {n_nodes} reaches, {n_edges} edges\")\n\nprint(f\"Total transfer links: {gs.tot_transfer}\")\n</code></pre> <p>This helps confirm that clusters remain balanced and that the transfer buffers stay manageable.</p>"},{"location":"concepts/multi_stage/#extremely-large-problem-sizes","title":"Extremely large problem sizes","text":"<p>Some routing problems are so large that the full output discharge can not be held into GPU memory at once. <code>diffroute</code> accomodates for this by allowing chunked computations using python generators:</p> <pre><code>runoff_generator =  per_subcluster_runoffs # List[torch.Tensor]\ndischarge = router.yield_(runoff_generator, gs) # Generator of output torch.Tensor discharge\n</code></pre> <p>We recommend using <code>diffhydro</code> for surch large problems to efficiently handle chunked runoff generation. </p>"},{"location":"concepts/single_stage/","title":"Basic Routing","text":"<p>Routing in DiffRoute is done by the forward function of the <code>LTIRouter</code> PyTorch module, which takes a <code>torch.Tensor</code> runoff and a <code>RivTree</code> description of the basin as inputs to compute an output routed <code>torch.Tensor</code> discharge.  <code>LTIRouter</code> is configured at initialization with routing hyper-parameters.  <code>RivTree</code> wraps a NetworkX <code>DiGraph</code> and the corresponding per-channel parameters in a GPU-ready format.</p> <p>This page builds on the Quickstart snippet with richer context: first dissecting the main router hyper-parameters, then detailing the <code>RivTree</code> structure, before walking through the internal execution stages that turn runoff into discharge.</p> <p>The router combines two internal phases: (1) it aggregates per-channel impulse response functions (IRFs) into a sparse routing kernel, and (2) it applies a causal block-sparse convolution against runoff time series. The sections below expand on each step with practical code snippets and configuration guidance.</p>"},{"location":"concepts/single_stage/#core-example","title":"Core example","text":"<p>The canonical example shared in the Overview and Quickstart pages forms the foundation for the more advanced notes below.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport torch\n\nfrom diffroute import RivTree, LTIRouter\n\nb = 2\nn = 20\nT = 100\ndevice = \"cuda:0\"\n\nG = nx.gn_graph(n)\nparams = pd.DataFrame({\"tau\": np.random.rand(n)}, index=G.nodes)\n\nriv_tree = RivTree(G, params=params, irf_fn=\"linear_storage\").to(device)\nrouter = LTIRouter(max_delay=48, dt=1)\n\nrunoff = torch.rand(b, n, T, device=device)\ndischarge = router(runoff, riv_tree)\n</code></pre> <p>This snippet builds a synthetic tree, attaches channel-level parameters, constructs a routing kernel once, and applies it to batched runoff in a differentiable manner. Replace the toy graph, parameter table, or device selection to match your own workflow.</p>"},{"location":"concepts/single_stage/#the-ltirouter-module","title":"The LTIRouter module","text":""},{"location":"concepts/single_stage/#main-parameters","title":"Main Parameters:","text":"<p>Two knobs govern most routing behaviour:</p> <ul> <li><code>max_delay</code>: Temporal support (expressed in routing steps) for the aggregated kernel. Set it large enough to cover the slowest travel time across your basin; anything shorter risks truncating downstream contributions.</li> <li><code>dt</code>: Ratio between the routing resolution and the native runoff resolution. With daily runoff, <code>dt = 1.0</code> keeps routing daily, while <code>dt = 1.0 / 24</code> lifts the computation to hourly resolution before optionally downsampling.</li> </ul>"},{"location":"concepts/single_stage/#secondary-parameters","title":"Secondary Parameters","text":"<p>Additional hyper-parameters are available for specialised tuning, but their defaults usually perform well:</p> <ul> <li><code>cascade</code>: Integer repeat count that cascades the IRF to emulate multi-reservoir responses.</li> <li><code>block_size</code>: Channel block size used when converting the sparse kernel to block-sparse form. Align this with your GPU\u2019s preferred tile size; the default value of 16 is a safe baseline.</li> <li><code>sampling_mode</code>: Strategy used by the <code>SubResolutionSampler</code> to reduce high-resolution kernels back to native resolution. <code>avg</code> emits window averages, while <code>sample</code> returns the last sample of each window.</li> <li><code>block_f</code>: Controls the frequency-domain aggregation batch size (FFT plus transitive closure). Larger values reduce kernel assembly passes at the expense of memory.</li> <li><code>include_index_diag</code> (defined on <code>RivTree</code>): When <code>False</code>, routed discharge is added to the original runoff, effectively including identity links. Leave it at <code>True</code> unless you explicitly need cumulative routing.</li> </ul>"},{"location":"concepts/single_stage/#the-rivtree-structure","title":"The RivTree structure","text":"<p><code>RivTree</code> materialises the river network in tensor form so the router can consume it efficiently. Instantiating a <code>RivTree</code> looks like:</p> <pre><code>riv_tree = RivTree(G, params=params, irf_fn=\"linear_storage\")\n</code></pre> <p>The <code>params</code> DataFrame must expose one column per IRF parameter required by the selected <code>irf_fn</code>. The authoritative list lives in <code>diffroute.irfs.IRF_PARAMS</code> and is summarised in the table below.</p> <p>Internally, <code>riv_tree</code> tracks three key tensors:</p> <ul> <li><code>riv_tree.nodes_idx</code>: <code>pandas.Series</code> mapping each NetworkX node label to a contiguous integer. Reorder runoff tensors according to this mapping before routing.</li> <li><code>riv_tree.edges</code>: <code>torch.Tensor</code> that stores the downstream successor indices for each channel in traversal order. The router uses it to build transitive closures.</li> <li><code>riv_tree.params</code>: <code>torch.Tensor</code> shaped <code>[n_channels, n_params]</code> containing the parameter matrix ordered consistently with <code>nodes_idx</code>.</li> </ul> <p>Additional buffers such as <code>riv_tree.path_cumsum</code> cache prefix sums used during block-sparse assembly, eliminating repeated graph traversals at execution time.</p>"},{"location":"concepts/single_stage/#built-in-irfs-and-expected-parameters","title":"Built-in IRFs and expected parameters","text":"<p>DiffRoute ships the following IRFs in <code>diffroute.irfs</code>.  Each channel in your river graph must provide the parameter vector listed here.  Parameters can be stored directly on NetworkX nodes (as in the quickstart) or supplied via a <code>pandas.DataFrame</code>.</p> IRF name Description Expected parameters <code>pure_lag</code> Unit hydrograph that delays runoff without attenuation <code>delay</code> <code>linear_storage</code> Discrete linear reservoir <code>tau</code> <code>nash_cascade</code> Closed-form cascade of <code>n</code> linear reservoirs (<code>n</code> set in <code>cascade</code>) <code>tau</code> <code>muskingum</code> Classical Muskingum channel routing <code>x</code>, <code>k</code> <code>hayami</code> Hayami diffusion wave approximation <code>D</code> (diffusivity), <code>L</code> (channel length), <code>c</code> (wave celerity)"},{"location":"concepts/single_stage/#registering-custom-routing-schemes","title":"Registering custom routing schemes","text":"<p><code>diffroute</code> suports easy integration of new custom LTI routing scheme through the <code>diffroute.irfs.register_irf(name, func, params)</code> utility function. This utility function takes as imput:</p> <ul> <li><code>name</code>: a string identifier to be passed to the <code>irf_fn</code> attribute of the <code>RivTree</code> initialization.</li> <li><code>func</code>: a callable that accepts <code>(params, time_window, dt)</code> and returns kernels shaped <code>[n_channels, window]</code></li> <li><code>params</code>: a tuple of string describing the per-channel routing parameter of the routing scheme to register (i.e. (<code>x</code>, <code>k</code>) for the Muskingum scheme)</li> </ul> <p>A tutorial notebook is provided showing the XXX. </p>"},{"location":"concepts/single_stage/#internals-of-the-routing-procedure","title":"Internals of the routing procedure","text":"<p>The forward function of <code>LTIRouter</code> is intentionally compact:</p> <pre><code>def forward(self, runoff, riv_tree, params=None):\n    kernel = self.aggregator(riv_tree, params)\n    block_sparse = kernel.to_block_sparse(self.block_size)\n    discharge = self.conv(runoff, block_sparse)\n    return discharge if riv_tree.include_index_diag else runoff + discharge\n</code></pre> <p>Execution proceeds in three stages:</p> <ul> <li>(i) IRF aggregation: gathers per-channel kernels, applies routing parameters (including optional overrides), and accumulates path responses into a sparse tensor.</li> <li>(ii) Block-sparse conversion: slices the aggregated kernel into GPU-friendly tiles controlled by <code>block_size</code>, caching the layout for fast re-use.</li> <li>(iii) Block-sparse convolution: multiplies the cached tiles against runoff tensors to generate routed discharge, optionally adding the original runoff if self-links are excluded.</li> </ul> <p>For research and development purposes, one can introspect the routing kernel produced by the aggregator before it is converted to block-sparse form.</p> <pre><code>import matplotlib.pyplot as plt\n\nkernel = router.aggregator(riv_tree)\n# Show size of the sparse kernel.\nprint(kernel.size, kernel.coords.shape, kernel.values.shape)\n\ndense_kernel = kernel.to_dense()\nirf = dense_kernel[1,0] # Path-transverse IRF from node 0 to node 1\nplt.plot(irf) # Plot the IRF\n</code></pre> <p>This is useful when debugging sparse connectivity, verifying kernel support, or exporting the kernel to custom tooling.</p>"},{"location":"concepts/single_stage/#dynamic-parameter-inputs","title":"Dynamic parameter inputs","text":"<p><code>RivTree</code> stores per-channel routing parameters in the tensor <code>riv_tree.params</code>, ordered exactly like <code>riv_tree.nodes_idx</code>. You can populate these values either by attaching attributes to your NetworkX graph or by supplying a <code>pandas.DataFrame</code> at construction time.</p> <p>Static routing scenarios can rely on the parameters embedded in the <code>RivTree</code>, but differentiable calibration often benefits from injecting learnable tensors into the forward pass.</p> <pre><code># Build a learnable tensor aligned with riv_tree.nodes_idx.\nlearnable_tau = torch.nn.Parameter(riv_tree.params.clone())\n\n# Route with dynamic parameters (must follow the node ordering)\ndischarge = router(runoff, riv_tree, params=learnable_tau)\n</code></pre> <p>As long as the tensor matches the shape and ordering dictated by <code>riv_tree.nodes_idx</code>, the aggregator swaps it in for the stored parameters. Refer to the Data Structures page for a deeper dive into <code>RivTree</code>, node ordering, and parameter handling, and explore <code>diffhydro</code> if you need end-to-end calibration workflows.</p>"},{"location":"examples/custom_irf/","title":"Defining a Custom IRF","text":"<p>DiffRoute lets you extend the library with custom impulse response functions (IRFs).  In this example, we will implement the Weibull-based Instantaneous Unit Hydrograph (IUH) as an example to demonstrate how to register such custom IRFs. The Weibull-based Instantaneous Unit Hydrograph (IUH) is a flexible probability-density IRF  that emerged from the probabilistic IUH literature as an alternative to linear reservoirs, Nash cascades, and Muskingum routing.  Studies such as Bhunya et al. (2008) and Nadarajah (2007) highlight the Weibull IUH\u2019s ability to reproduce skewed hydrograph shapes using only the shape (<code>k</code>) and scale (<code>\u03bb</code>) parameters, with an optional onset delay. </p> <p>We will implement the IRF in PyTorch, register it within <code>diffroute</code>, and demonstrate how to route runoff using the new kernel.</p>"},{"location":"examples/custom_irf/#1-import-dependencies","title":"1. Import dependencies","text":"<p>Bring in PyTorch along with the DiffRoute helpers, plus the scientific Python stack used to synthesise a random river network and parameter table.</p> <pre><code>import torch\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\n\nfrom diffroute import LTIRouter, RivTree, register_irf\n</code></pre>"},{"location":"examples/custom_irf/#2-implement-the-weibull-iuh","title":"2. Implement the Weibull IUH","text":"<p>The IRF signature follows DiffRoute\u2019s convention: <code>(params, time_window, dt) -&gt; tensor[n_reaches, window]</code>. Each row of the returned tensor must be normalised to unit mass. We expose three parameters\u2014<code>shape</code>, <code>scale</code>, and <code>onset</code>\u2014to mirror the standard Weibull PDF with an optional translation.</p> <pre><code>def weibull_irf(params, time_window, dt):\n    \"\"\"\n    params[:, 0] -&gt; shape (k)     : controls peak sharpness\n    params[:, 1] -&gt; scale (lam)   : stretches the hydrograph in time\n    params[:, 2] -&gt; onset (t0)    : optional delay before response\n    \"\"\"\n    shape = params[:, 0].unsqueeze(1)\n    scale = params[:, 1].unsqueeze(1)\n    onset = params[:, 2].unsqueeze(1) if params.size(1) &gt; 2 else torch.zeros_like(shape)\n\n    steps = max(1, int(round(time_window / dt)))\n    t = torch.arange(1, steps + 1, device=params.device, dtype=params.dtype).unsqueeze(0) * dt\n\n    te = (t - onset).clamp(min=0)\n    eps = torch.finfo(params.dtype).eps\n    z = te / scale.clamp(min=eps)\n    mask = (te &gt; 0).to(params.dtype)\n\n    pulse = mask * (shape / scale.clamp(min=eps)) * torch.pow(z.clamp(min=eps), shape - 1) * torch.exp(-torch.pow(z, shape))\n    pulse = pulse / (pulse.sum(dim=-1, keepdim=True) + eps)\n    return pulse\n</code></pre>"},{"location":"examples/custom_irf/#3-register-the-irf","title":"3. Register the IRF","text":"<p><code>register_irf</code> adds the implementation to DiffRoute\u2019s global registry and associates human-readable parameter names with the kernel. These labels must match the attributes you store on NetworkX nodes (or columns in your <code>pandas.DataFrame</code>), so keep the naming consistent.</p> <pre><code>register_irf(\n    \"weibull_iuh\",\n    weibull_irf,\n    params=[\"shape\", \"scale\", \"onset\"]\n)\n</code></pre> <p>After registration, <code>RivTree</code> knows that any network using <code>irf_fn=\"weibull_iuh\"</code> requires the parameters <code>shape</code>, <code>scale</code>, and <code>onset</code>, and it will pack them in that exact order when building tensors for the router.</p>"},{"location":"examples/custom_irf/#4-build-a-river-network-with-weibull-parameters","title":"4. Build a river network with Weibull parameters","text":"<p>Create a random river tree using the same <code>nx.gn_graph</code> helper employed in the Overview example. Store the Weibull parameters in a <code>pandas.DataFrame</code>, with one independently generated column per parameter. Passing the DataFrame to <code>RivTree</code> keeps the parameter ordering consistent with the registration labels.</p> <pre><code>n_reaches = 20\nrng = np.random.default_rng(seed=0)\nG = nx.gn_graph(n_reaches, seed=0)\n\nparam_df = pd.DataFrame(\n    {\n        \"shape\": rng.uniform(1.2, 2.5, size=n_reaches),\n        \"scale\": rng.uniform(10.0, 24.0, size=n_reaches),\n        \"onset\": rng.uniform(0.0, 4.0, size=n_reaches),\n    },\n    index=G.nodes\n)\n\nriv_tree = RivTree(\n    G,\n    params=param_df,\n    irf_fn=\"weibull_iuh\",\n    include_index_diag=False\n)\n</code></pre>"},{"location":"examples/custom_irf/#5-route-runoff-with-the-custom-kernel","title":"5. Route runoff with the custom kernel","text":"<p>Instantiate <code>LTIRouter</code> with the same <code>irf_fn</code> name and feed it a sample runoff tensor. DiffRoute automatically looks up the registered kernel and hands the per-reach parameter tensor\u2014ordered as <code>[\"shape\", \"scale\", \"onset\"]</code>\u2014to the Weibull IUH implementation.</p> <pre><code>router = LTIRouter(\n    max_delay=96,\n    dt=1.0,\n    irf_fn=\"weibull_iuh\"\n)\n\nrunoff = torch.rand(1, len(riv_tree), 168)\ndischarge = router(runoff, riv_tree)\nprint(discharge.shape)\n</code></pre>"},{"location":"examples/custom_irf/#key-points","title":"Key points","text":"<ul> <li>Custom IRFs must implement the <code>(params, time_window, dt)</code> interface and return <code>[n_reaches, window]</code> kernels normalised to unit mass.</li> <li><code>register_irf</code> couples the kernel implementation with parameter labels; these labels must match the attributes used when constructing <code>RivTree</code>.</li> <li>Using the same <code>irf_fn</code> identifier during registration, network construction, and routing ensures DiffRoute wires the correct parameters into the custom kernel.</li> </ul>"},{"location":"examples/rapid/","title":"Routing a RAPID Project","text":"<p>This tutorial walks through configuring DiffRoute against RAPID inputs, covering graph ingestion, staged routing, and preparation of runoff forcings stored as NetCDF.  Each step mirrors how you would wire DiffRoute into an operational RAPID deployment. In this tutorial, we suppose a standard RAPID project with CSV definition files (<code>rapid_connect.csv</code>, <code>k.csv</code>, <code>x.csv</code>, <code>riv_bas_id.csv</code>)  and NetCDF input runoff file (<code>input.nc</code>) stored in a same directory.</p>"},{"location":"examples/rapid/#1-import-dependencies","title":"1. Import dependencies","text":"<p>Start by importing the utilities needed to read RAPID configurations, manage paths, load NetCDF runoff with <code>xarray</code>, and move data into PyTorch tensors.</p> <pre><code>from pathlib import Path\nimport torch\nimport xarray as xr\n\nfrom diffroute import LTIStagedRouter\nfrom diffroute.io import read_rapid_graph\n</code></pre>"},{"location":"examples/rapid/#2-load-the-rapid-configuration","title":"2. Load the RAPID configuration","text":"<p><code>read_rapid_graph</code> parses the standard RAPID CSV files (<code>rapid_connect.csv</code>, <code>k.csv</code>, <code>x.csv</code>, <code>riv_bas_id.csv</code>)  and materialises them as either a <code>RivTree</code> or a <code>RivTreeCluster</code>, depending on the partitioning thresholds you provide.</p> <pre><code>vpu_root = Path(\"/data/rapid/VPU1201\")\ngs = read_rapid_graph(\n    vpu_root,\n    plength_thr=30_000,  # optional: segment the network\n    node_thr=600\n)\n</code></pre>"},{"location":"examples/rapid/#3-configure-the-staged-router","title":"3. Configure the staged router","text":"<p><code>LTIStagedRouter</code> accepts the same high-level hyper-parameters as <code>LTIRouter</code> while handling clustered networks transparently.  You can tune <code>max_delay</code> and <code>dt</code> to match your hydrological assumptions.</p> <pre><code>router = LTIStagedRouter(\n    max_delay=96,\n    dt=1.0,\n)\n</code></pre>"},{"location":"examples/rapid/#4-load-rapid-runoff-forcings-from-netcdf","title":"4. Load RAPID runoff forcings from NetCDF","text":"<p>Load the NetCDF input runoff into an xarray and order the catchments according to the <code>gs.node_idxs</code> ordering. Then convert the xarray data into the <code>[batch, catchments, time]</code> tensor layout expected by the router</p> <pre><code>runoff_da = xr.open_dataarray(vpu_root / \"input.nc\")\nrunoff_da = runoff_ds.transpose(\"reach_id\", \"time\").sel(reach_id=gs.node_idxs.index)\nrunoff = torch.from_numpy(runoff_da.values).unsqueeze(0).to(device)\n</code></pre>"},{"location":"examples/rapid/#5-route-the-forcings","title":"5. Route the forcings","text":"<p>Once the forcings follow the expected device and layout, invoke the router. </p> <pre><code>discharge = router(runoff, gs)\nprint(discharge.shape)\n</code></pre> <ul> <li>The pipeline above is fully differentiable, so you can backpropagate through runoff inputs or Muskingum parameters when calibrating against observations.</li> <li>Swap the synthetic NetCDF for your production runoff forcings to reproduce an end-to-end RAPID routing workflow with DiffRoute.</li> </ul>"}]}